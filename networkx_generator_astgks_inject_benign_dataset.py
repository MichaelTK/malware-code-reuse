import pickle
import os
import pandas as pd
import sys
from collections import Counter

DICTIONARIES_PATH = "/home/k1462425/Documents/Research/malware-code-reuse-scripts/SourcererCC-artifact/sourc_results_ufsf/"
INJECTED_DATASET_PATH = "/home/k1462425/Documents/Research/malware-code-reuse-scripts/50KC_flat_with_cloned_postmethod_zips/astgks_clones/"

UFSF_DATASET_PATH = "/home/k1462425/Documents/Research/malware-code-reuse-scripts/ufsf_java_repos_flat/ufsf_java_repos_flat/"
KC50_DATASET_PATH = "/home/k1462425/Documents/Research/malware-code-reuse-scripts/50KC_flat/"

PRIORITISED_TYPES = ['spoof','ransomwar','backdoor','keylogg','rootkit','ddos']
blockids_cloneblockids_evaluation = []
counter = 0

def main():
    projectids_blockids = load_dictionary(DICTIONARIES_PATH+'projectids_blockids_java.pkl')
    blockids_filepaths = load_dictionary(DICTIONARIES_PATH+'blockids_filepaths_java.pkl')
    projectids_numblocks = load_dictionary(DICTIONARIES_PATH+'projectids_numblocks_java.pkl')
    projectpairs_clonepercentage = load_dictionary(DICTIONARIES_PATH+'projectpairs_clonepercentage_java.pkl')
    blockids_cloneblockids = load_dictionary(DICTIONARIES_PATH+'blockids_cloneblockids_java.pkl')
    projectids_cloneprojectids = load_dictionary(DICTIONARIES_PATH+'projectids_cloneprojectids_java.pkl')
    projectids_names = load_dictionary(DICTIONARIES_PATH+'projectids_names_java.pkl')
    # repolinks_types_withmultilabel = load_dictionary(DICTIONARIES_PATH+'repolinks_types_withmultilabel.pkl')
    blockids_cloneblockids_similarities = pd.read_pickle(DICTIONARIES_PATH+'blockids_cloneblockids_similarities.pkl')
    blockids_cloneblockids_similarities_filtered = pd.read_pickle(DICTIONARIES_PATH+'blockids_cloneblockids_similarities_filtered_java.pkl')
    blockids_locations = pd.read_pickle(DICTIONARIES_PATH+"blockids_locations_java.pkl")
    projectnames_ids = {}
    for projectid in projectids_names:
        projectnames_ids[projectids_names[projectid]] = projectid

    print(blockids_cloneblockids_similarities_filtered)

    projectnames_blockids = get_projectnames_blockids(projectids_blockids,projectids_names)
    blockids_projectnames = get_blockids_projectnames(projectnames_blockids)
    # projectnames_blockids_50kc_ufsf = get_projectnames_blockids(projectids_blockids_50kc_ufsf,projectids_names_50kc_ufsf)
    # blockids_projectnames_50kc_ufsf = get_blockids_projectnames(projectnames_blockids_50kc_ufsf)

    blockids_to_copy = set([])
    x = 0
    for triad in blockids_cloneblockids_similarities_filtered:
        blockids_to_copy.add(triad[0])
        blockids_to_copy.add(triad[1])
        x += 1
        print(str(x)+"/"+str(len(blockids_cloneblockids_similarities_filtered)))
    print(len(blockids_to_copy))
    blockids_to_copy = list(blockids_to_copy)
    if os.path.exists(DICTIONARIES_PATH+'blockids_to_copy.pkl'):
        os.remove(DICTIONARIES_PATH+'blockids_to_copy.pkl')
    with open(DICTIONARIES_PATH+'blockids_to_copy.pkl', 'wb') as f:
        pickle.dump(blockids_to_copy, f)

    paths_and_blockids = inject_into_dataset(blockids_to_copy,blockids_locations)
    if os.path.exists(DICTIONARIES_PATH+'paths_and_blockids.pkl'):
        os.remove(DICTIONARIES_PATH+'paths_and_blockids.pkl')
    with open(DICTIONARIES_PATH+'paths_and_blockids.pkl', 'wb') as f:
        pickle.dump(paths_and_blockids, f)



def load_dictionary(path):
    b = {}
    with open(path, 'rb') as handle:
        b = pickle.load(handle)
    return b

def get_weight(projectid,cloneid,projectpairs_clonepercentage):
    weight1 = projectpairs_clonepercentage[(projectid,cloneid)]
    weight2 = projectpairs_clonepercentage[(cloneid,projectid)]
    theweight = 0
    if weight1 > weight2:
        theweight = weight1
    elif weight2 > weight1:
        theweight = weight2
    else:
        theweight = weight1
    return theweight

def get_num_subgraphs_sf_mixed_uf(subgraphs,nodenames_malwaretypes,projectids_names):
    num_subgraphs_sf = 0
    num_subgraphs_mixed = 0
    num_subgraphs_uf = 0
    num_nodes_in_sf_subgraphs = 0
    num_nodes_in_uf_subgraphs = 0
    num_nodes_in_mixed_subgraphs = 0
    uf_subgraphs = []
    subgraphs_debug = []
    for subgraph in subgraphs:
        sf = False
        uf = False
        if len(subgraph.nodes) > 1:
            print(subgraph.nodes)
            subgraphs_debug.append(subgraph)
            for node in subgraph.nodes:
                nodename = projectids_names[node]
                label = nodenames_malwaretypes[nodename]
                print(nodename)
                print(label)
                # label = node['label']
                if label == ['unknown']:
                    uf = True
                else:
                    sf = True
            if sf and not uf:
                num_subgraphs_sf += 1
                num_nodes_in_sf_subgraphs += len(subgraph.nodes)
            elif uf and not sf:
                num_subgraphs_uf += 1
                num_nodes_in_uf_subgraphs += len(subgraph.nodes)
                uf_subgraphs.append(subgraph)
            elif sf and uf:
                num_subgraphs_mixed += 1
                num_nodes_in_mixed_subgraphs += len(subgraph.nodes)

    print(len(subgraphs_debug))
    print("Mean number of nodes in only sf subgraphs: "+str(num_nodes_in_sf_subgraphs/num_subgraphs_sf))
    print("Mean number of nodes in only uf subgraphs: "+str(num_nodes_in_uf_subgraphs/num_subgraphs_uf))
    print("Mean number of nodes in only mixed subgraphs: "+str(num_nodes_in_mixed_subgraphs/num_subgraphs_mixed))

    num_forked_clusters = 0
    num_uf_clusters_2size = 0
    for subgraph in uf_subgraphs:
        if len(subgraph.nodes) < 3:
            num_uf_clusters_2size += 1
            for node in subgraph.nodes:
                nodename = projectids_names[node]
                for node2 in subgraph.nodes:
                    nodename2 = projectids_names[node2]
                    if nodename != nodename2:
                        if nodename in nodename2 or nodename2 in nodename:
                            num_forked_clusters += 1

    num_forked_clusters3 = 0
    num_uf_clusters_3size = 0
    for subgraph in uf_subgraphs:
        if len(subgraph.nodes) == 3:
            num_uf_clusters_3size += 1
            for node in subgraph.nodes:
                nodename = projectids_names[node]
                for node2 in subgraph.nodes:
                    nodename2 = projectids_names[node2]
                    for node3 in subgraph.nodes:
                        nodename3 = projectids_names[node3]
                        if nodename != nodename2 and nodename != nodename3 and nodename2 != nodename3:
                            if (nodename in nodename2 or nodename in nodename3) or (nodename2 in nodename or nodename2 in nodename3) or (nodename3 in nodename or nodename3 in nodename2):
                                num_forked_clusters3 += 1

    print(num_forked_clusters)
    print(num_uf_clusters_2size)
    print("-----------")
    print(num_forked_clusters3)
    print(num_uf_clusters_3size)

    return num_subgraphs_sf,num_subgraphs_mixed,num_subgraphs_uf


def get_repodirs_and_sizes(all_projects,projectids_numblocks,projectnames_ids):
    repodirs_and_sizes = {}
    for project in all_projects:
        size = projectids_numblocks[projectnames_ids[project]]
        repodirs_and_sizes[project] = size
    return repodirs_and_sizes

def get_repolinks_postmethod(directories_and_repolinks,all_projects):
    repolinks = []
    dirnames_and_repolinks = {}
    for directory in directories_and_repolinks:
        dirnames_and_repolinks[directory.split("/")[-1]] = directories_and_repolinks[directory]
    for project in all_projects:
        repolinks.append(dirnames_and_repolinks[project])
    repolinks = list(set(repolinks))
    return repolinks

def takeout_1d(blockids_reused_in_benign,blockids_cloneblockids_similarities_filtered):
    blockids_cloneblockids_similarities_filtered2 = []
    to_remove = []
    for triad in blockids_cloneblockids_similarities_filtered:
        if not (triad[0] in blockids_reused_in_benign) and not (triad[1] in blockids_reused_in_benign):
            blockids_cloneblockids_similarities_filtered2.append(triad)
    return blockids_cloneblockids_similarities_filtered2


def discard_blockids_same_repos(blockids_reused_in_benign,blockids_locations,blockids_locations_50kc_ufsf,blockids_astgks_and_50kc,blockids_projectnames,blockids_projectnames_50kc_ufsf):
    blockids_50kc_and_astgks = {}
    blockids_to_remove = []
    projectnames_in_common = []
    for blockid in blockids_astgks_and_50kc:
        blockids_50kc_and_astgks[blockids_astgks_and_50kc[blockid]] = blockid
    for blockid in blockids_reused_in_benign:
        projectname_astgks = blockids_projectnames[blockid]
        blockid_50kc = blockids_astgks_and_50kc[blockid]
        projectname_50kc = blockids_projectnames_50kc_ufsf[blockid_50kc]
        if projectname_astgks == projectname_50kc:
            blockids_to_remove.append(blockid)
            projectnames_in_common.append(projectname_astgks)

    projectnames_in_common = list(set(projectnames_in_common))
    print("Number of projects in common: "+str(len(projectnames_in_common)))
    print(projectnames_in_common)

    for blockid in blockids_to_remove:
        blockids_reused_in_benign.remove(blockid)
    return blockids_reused_in_benign


def get_blockids_reused_in_benign(paths_and_blockids,blockids_cloneblockids_50kc_ufsf,blockids_locations_50kc_ufsf):
    blockids_astgks_and_50kc = {}
    blockids_reused_in_benign = []
    for blockid in blockids_cloneblockids_50kc_ufsf:
        for cloneblockid in blockids_cloneblockids_50kc_ufsf[blockid]:
            astgks_blockid = get_astgks_blockid(blockid,paths_and_blockids,blockids_locations_50kc_ufsf)
            astgks_cloneblockid = get_astgks_blockid(blockid,paths_and_blockids,blockids_locations_50kc_ufsf)
            if astgks_blockid != 0:
                blockids_reused_in_benign.append(astgks_blockid)
                blockids_astgks_and_50kc[astgks_blockid] = blockid
            if astgks_cloneblockid != 0:
                blockids_reused_in_benign.append(astgks_cloneblockid)
                blockids_astgks_and_50kc[astgks_cloneblockid] = cloneblockid
    blockids_reused_in_benign = list(set(blockids_reused_in_benign))

    return blockids_reused_in_benign,blockids_astgks_and_50kc

def get_astgks_blockid(blockid,paths_and_blockids,blockids_locations_50kc_ufsf):
    location = blockids_locations_50kc_ufsf[blockid]
    file = ""
    # print(location[0])
    # sys.exit(0)
    if "astgks" in location[0]:
        # print("done")
        # sys.exit(0)
        file = location[0]
        dir = file.split("/")[-2]
        filename = file.split("/")[-1]
        file = "/media/k1462425/BackupsDrive/50KC_flat_with_cloned_postmethod_zips/"+dir+"/"+filename
        file = file[:-1]

    # print(paths_and_blockids)
    # '/media/k1462425/BackupsDrive/50KC_flat_with_cloned_postmethod_zips/astgks_clones/sample20307.java'
    astgks_blockid = 0
    if len(file) > 0:
        # print(file)
        # sys.exit(0)
        astgks_blockid = paths_and_blockids[file]
    return astgks_blockid

def inject_into_dataset(blockids_to_copy,blockids_locations):
    paths_and_blockids = {}
    if not os.path.exists(INJECTED_DATASET_PATH):
        os.mkdir(INJECTED_DATASET_PATH)
    x = 0
    for blockid in blockids_to_copy:
        with open(INJECTED_DATASET_PATH+"sample"+str(x)+".java",'a+') as fp:
            fp.write("public class Dummy {\n")
            fp.write(get_code_by_location(blockids_locations[blockid]))
            fp.write("\n}")
            paths_and_blockids[INJECTED_DATASET_PATH+"sample"+str(x)+".java"] = blockid
        print("Wrote to: "+INJECTED_DATASET_PATH+"sample"+str(x)+".java "+str(x))
        x += 1
    return paths_and_blockids


def get_blockids_to_copy_unique(blockids_to_copy,blockids_locations):
    duplicates = blockids_to_copy
    unique_dups = []
    x = 0
    for x in range(len(duplicates)):
        found = False
        dup1 = duplicates[x]
        for y in range(len(unique_dups)):
            dup2 = unique_dups[y]
            code1 = get_code_by_location(blockids_locations[dup1])
            code2 = get_code_by_location(blockids_locations[dup2])
            # distance = Levenshtein.distance(code1,code2)
            # if distance < 10:
            if code1 == code2:
                found = True
        if not found:
            unique_dups.append(dup1)
        x += 1
        if x % 100 == 0:
            print(str(x)+"/"+str(len(duplicates)))
            print("Length of unique_dups: "+str(len(unique_dups)))

    unique_dups = list(set(unique_dups))

    return unique_dups


def get_unique_funcs_blockids(blockids_cloneblockids_similarities_filtered,blockids_locations):
    unique_funcs_blockids = []
    unique_dups = []
    blockids_done = []
    for x in range(len(blockids_cloneblockids_similarities_filtered)):
        found = False
        dup1 = blockids_cloneblockids_similarities_filtered[x][0]
        for y in range(len(unique_dups)):
            dup2 = unique_dups[y]
            code1 = get_code_by_location(blockids_locations[dup1])
            code2 = get_code_by_location(blockids_locations[dup2])
            # distance = Levenshtein.distance(code1,code2)
            # if distance < 10:
            if code1 == code2:
                found = True
        if not found:
            unique_dups.append(dup1)
        if x % 100 == 0:
            print(str(x)+"/"+str(len(blockids_cloneblockids_similarities_filtered)))
            print("Length of unique_dups: "+str(len(unique_dups)))
        blockids_done.append(dup1)

    for x in range(len(blockids_cloneblockids_similarities_filtered)):
        found = False
        dup1 = blockids_cloneblockids_similarities_filtered[x][1]
        if dup1 not in blockids_done:
            for y in range(len(unique_dups)):
                dup2 = unique_dups[y]
                code1 = get_code_by_location(blockids_locations[dup1])
                code2 = get_code_by_location(blockids_locations[dup2])
                # distance = Levenshtein.distance(code1,code2)
                # if distance < 10:
                if code1 == code2:
                    found = True
            if not found:
                unique_dups.append(dup1)
        if x % 100 == 0:
            print(str(x)+"/"+str(len(blockids_cloneblockids_similarities_filtered)))
            print("Length of unique_dups: "+str(len(unique_dups)))
        blockids_done.append(dup1)

    unique_dups = list(set(unique_dups))
    return unique_dups


def takeout(duplicates,blockids_cloneblockids_similarities_filtered):
    triads_to_remove = []
    x = 0
    for duplicate in duplicates:
        for triad in blockids_cloneblockids_similarities_filtered:
            if (duplicate[0] == triad[0] or duplicate[0] == triad[1]):
                triads_to_remove.append(triad)
        x += 1
        print(str(x)+"/"+str(len(duplicates)))
    x = 0
    for triad in triads_to_remove:
        try:
            blockids_cloneblockids_similarities_filtered.remove(triad)
        except Exception as e:
            print(e)
        x += 1
        print(str(x)+"/"+str(len(triads_to_remove)))
    return blockids_cloneblockids_similarities_filtered


def get_blockids_cloneblockids_similarities(blockids_filepaths,blockids_cloneblockids,blockids_locations):
    blockids_cloneblockids_similarities = []
    num = 0
    for blockid in blockids_cloneblockids:
        for cloneblockid in blockids_cloneblockids[blockid]:
            num += 1
    print("Number of blocks to get through: "+str(num))

    count = 0
    for blockid in blockids_cloneblockids:
        for cloneblockid in blockids_cloneblockids[blockid]:
            triad = (blockid,cloneblockid,1)
            blockids_cloneblockids_similarities.append(triad)
            count += 1
            if count % 10000 == 0:
                print(str(count)+"/"+str(num))

    return blockids_cloneblockids_similarities

def get_malware_types_links(edges,projectids_names,nodenames_malwaretypes):
    malware_types_links = {}
    for edgepair in edges:
        projectid = edgepair[0]
        cloneprojectid = edgepair[1]
        malware_types1 = nodenames_malwaretypes[projectids_names[projectid]]
        malware_types2 = nodenames_malwaretypes[projectids_names[cloneprojectid]]
        for mtype in malware_types1:
            for mtype2 in malware_types2:
                if (mtype,mtype2) in malware_types_links.keys():
                    malware_types_links[(mtype,mtype2)] += 1
                else:
                    malware_types_links[(mtype,mtype2)] = 0

                if (mtype2,mtype) in malware_types_links.keys():
                    malware_types_links[(mtype2,mtype)] += 1
                else:
                    malware_types_links[(mtype2,mtype)] = 0

    malware_types_links = trim_malware_types_links(malware_types_links)
    return malware_types_links

def trim_malware_types_links(malware_types_links):
    to_remove = []
    for elem in malware_types_links:
        if elem[0] == elem[1]:
            to_remove.append(elem)
        if elem[0] == 'unknown' or elem[1] == 'unknown':
            to_remove.append(elem)
    to_remove = list(set(to_remove))
    for toremove in to_remove:
        del malware_types_links[toremove]
    return malware_types_links


def get_code_of_projectid_pair(projectid,cloneprojectid,projectids_blockids,blockids_filepaths,blockids_locations,blockids_cloneblockids,blockids_projectids,blockids_cloneblockids_similarities_filtered):
    global blockids_cloneblockids_evaluation
    global counter
    blockid = -1
    blockids = projectids_blockids[projectid]
    blockids_filtered = get_blockids_filtered(blockids_cloneblockids_similarities_filtered)
    for blockid in blockids:
        if blockid in blockids_filtered:
            if blockid in blockids_cloneblockids.keys():
                cloneblockids = blockids_cloneblockids[blockid]
                for cloneblockid in cloneblockids:
                    if cloneprojectid in blockids_projectids[cloneblockid]:
                        # print_code(blockid,blockids_filepaths,blockids_locations)
                        code = get_code_to_print(blockid,blockids_filepaths,blockids_locations)
                        blockids_cloneblockids_evaluation.append((blockid,cloneblockid))
                        # print("+++++++++++++++++++++++++++")
                        # print("+++++++++++++++++++++++++++")
                        # print("+++++++++++++++++++++++++++")
                        counter += 1
                        # print(counter)
                        return code

def get_blockids_filtered(blockids_cloneblockids_similarities_filtered):
    blockids_filtered = []
    for triad in blockids_cloneblockids_similarities_filtered:
        blockids_filtered.append(triad[0])
        blockids_filtered.append(triad[1])
    blockids_filtered = list(set(blockids_filtered))
    return blockids_filtered

def print_code(blockid,blockids_filepaths,blockids_locations):
    path1 = blockids_filepaths[blockid]
    dir1 = path1.split("/")[-2]
    file1 = path1.split("/")[-1][:-1]
    path1 = "/media/k1462425/BackupsDrive/ufsf_java_repos_flat/ufsf_java_repos_flat/"+dir1+"/"+file1
    print(get_code(path1,blockids_locations[blockid][1]))

def get_code_to_print(blockid,blockids_filepaths,blockids_locations):
    path1 = blockids_filepaths[blockid]
    dir1 = path1.split("/")[-2]
    file1 = path1.split("/")[-1][:-1]
    path1 = "/media/k1462425/BackupsDrive/ufsf_java_repos_flat/ufsf_java_repos_flat/"+dir1+"/"+file1
    return get_code(path1,blockids_locations[blockid][1])


def get_code(path,line_locs):
    lines = []
    line = ""
    try:
        with open(path,'r') as fp:
            try:
                line = fp.readline()
                lines.append(line)
                while line:
                    line = fp.readline()
                    lines.append(line)
            except Exception as e:
                return None
    except Exception as e:
        return None

    lines_pertaining = lines[int(line_locs[0])-1:int(line_locs[1])]
    # code = "public class main {\n"
    # code = code + "".join(lines_pertaining)
    # code = code + "\n}"
    code = "".join(lines_pertaining) + "}"
    return code

def get_blockids_numprojects(blockids_projectnames_appearing_in):
    blockids_numprojects = {}
    for blockid in blockids_projectnames_appearing_in:
        blockids_numprojects[blockid] = len(blockids_projectnames_appearing_in[blockid])
    return blockids_numprojects

def get_blockids_projectnames_appearing_in(blockids_cloneblockids_similarities,blockids_projectnames):
    blockids_projectnames_appearing_in = {}
    for triad in blockids_cloneblockids_similarities:
        if float(triad[2]) >= 0.87:
            blockids_projectnames_appearing_in[triad[0]] = set([])
            blockids_projectnames_appearing_in[triad[1]] = set([])
    for triad in blockids_cloneblockids_similarities:
        if float(triad[2]) >= 0.87:
            blockids_projectnames_appearing_in[triad[0]].add(blockids_projectnames[triad[0]])
            blockids_projectnames_appearing_in[triad[0]].add(blockids_projectnames[triad[1]])
            blockids_projectnames_appearing_in[triad[1]].add(blockids_projectnames[triad[1]])
            blockids_projectnames_appearing_in[triad[1]].add(blockids_projectnames[triad[0]])
    return blockids_projectnames_appearing_in

def get_blockids_projectnames(projectnames_blockids):
    blockids_projectnames = {}
    for projectname in projectnames_blockids:
        blockids = projectnames_blockids[projectname]
        for blockid in blockids:
            blockids_projectnames[blockid] = projectname
    return blockids_projectnames

def get_projectnames_blockids(projectids_blockids,projectids_names):
    projectnames_blockids = {}
    for projectid in projectids_blockids:
        projectnames_blockids[projectids_names[projectid]] = projectids_blockids[projectid]
    return projectnames_blockids

def get_blockids_numclones(blockids_cloneblockids_similarities,blockids_projectnames):
    blockids_numclones = {}
    for triad in blockids_cloneblockids_similarities:
        if blockids_projectnames[triad[0]] != blockids_projectnames[triad[1]]:
            if float(triad[2]) >= 0.87:
                blockids_numclones[triad[0]] = 0
                blockids_numclones[triad[1]] = 0
    for triad in blockids_cloneblockids_similarities:
        if blockids_projectnames[triad[0]] != blockids_projectnames[triad[1]]:
            if float(triad[2]) >= 0.87:
                blockids_numclones[triad[0]] += 1
                blockids_numclones[triad[1]] += 1
    return blockids_numclones

def get_most_cloned_blockids_thresholded(blockids_cloneblockids_similarities):
    for y in range(len(blockids_cloneblockids_similarities)):
        for x in range(len(blockids_cloneblockids_similarities))-1:
            if float(blockids_cloneblockids_similarities[x][2]) < float(blockids_cloneblockids_similarities[x+1][2]):
                temp = blockids_cloneblockids_similarities[x+1]
                blockids_cloneblockids_similarities[x+1] = blockids_cloneblockids_similarities[x]
                blockids_cloneblockids_similarities[x] = temp
    return blockids_cloneblockids_similarities

def get_code_by_location(location):
    path = location[0][1:-1]
    suffix = path.split("/")[-2:]
    suffix = "/".join(suffix)
    prefix = UFSF_DATASET_PATH
    filepath = prefix + suffix
    lines = []
    with open(filepath,'r') as fp:
        line = fp.readline()
        lines.append(line)
        while line:
            line = fp.readline()
            lines.append(line)

    selected_lines = lines[int(location[1][0])-1:int(location[1][1])]
    code = "".join(selected_lines)
    return code

def get_code_by_location_50kc(location):
    path = location[0][1:-1]
    suffix = path.split("/")[-2:]
    suffix = "/".join(suffix)
    prefix = KC50_DATASET_PATH
    filepath = prefix + suffix
    lines = []
    with open(filepath,'r',encoding='latin1') as fp:
        line = fp.readline()
        lines.append(line)
        while line:
            line = fp.readline()
            lines.append(line)

    selected_lines = lines[int(location[1][0])-1:int(location[1][1])]
    code = "".join(selected_lines)
    return code


def get_code_by_location_50kc_ufsf(location):
    path = location[0][1:-1]
    suffix = path.split("/")[-2:]
    suffix = "/".join(suffix)
    prefix = "/media/k1462425/BackupsDrive/50KC_flat_with_cloned_postmethod/"
    filepath = prefix + suffix
    lines = []
    with open(filepath,'r',encoding='latin1') as fp:
        line = fp.readline()
        lines.append(line)
        while line:
            line = fp.readline()
            lines.append(line)

    selected_lines = lines[int(location[1][0])-1:int(location[1][1])]
    code = "".join(selected_lines)
    return code


def truncate_and_normalize(projectpairs_edgeweights):
    toremove = []
    # print(projectpairs_edgeweights)
    # sys.exit(0)
    for projectpair in projectpairs_edgeweights:
        # print(projectpairs_edgeweights[projectpair])
        if projectpairs_edgeweights[projectpair] < 0.87:
            toremove.append(projectpair)
    for pair in toremove:
        del projectpairs_edgeweights[pair]

    # print(projectpairs_edgeweights)
    # print("donk")
    # sys.exit(0)

    # for projectpair in projectpairs_edgeweights:
    #     if projectpairs_edgeweights[projectpair] == 1:
    #         projectpairs_edgeweights[projectpair] = 0.999999

    # projectpairs_edgeweights = normalize(projectpairs_edgeweights)

    # print(projectpairs_edgeweights)
    # print("dink")
    # sys.exit(0)
    return projectpairs_edgeweights

def normalize(d, target=1.0):
    raw = sum(d.values())
    factor = target/raw
    return {key:value*factor for key,value in d.items()}

def plot_timelines(repos_and_timespans_dates,dates,names):
    plt.rcParams.update({'font.size': 28})

    cmap = get_cmap(len(names)+1)

    for x in range(len(dates)):
        plt.plot(dates[x], [x+1,x+1], color=cmap(x),marker='o',markersize=12, label=names[x])

    plt.xlabel('Time')
    plt.ylim(0, len(dates)+1)
    #plt.yscale('log')
    plt.legend(loc='best')
    plt.show()

def get_cmap(n, name='hsv'):
    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct
    RGB color; the keyword argument name must be a standard mpl colormap name.'''
    return plt.cm.get_cmap(name, n)

def get_repodirs_and_timespans(repos_and_timespans):
    repodirs_and_timespans = {}
    for repo in repos_and_timespans:
        repodirs_and_timespans[repo.split("/")[-1]] = repos_and_timespans[repo]
    return repodirs_and_timespans


main()
