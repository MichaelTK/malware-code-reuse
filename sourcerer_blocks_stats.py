import os
import pickle
import pandas as pd
import zipfile
import sys
import javalang
from grakel.kernels import WeisfeilerLehman, VertexHistogram
from grakel import Graph
from javalang.ast import Node
#from tree import ASTNode, BlockNode
import string
import random

#BLOCK_LEVEL_PATH = "/scratch/michal/SourcererCC/SourcererCC/tokenizers/block-level/"
#PAIRS_PATH = "/scratch/michal/SourcererCC/SourcererCC/clone-detector/results_python_100.pairs"
#DICTIONARY_SAVE_PATH = "/scratch/michal/sourcerercc_ufsf_python100_dictionaries/"

#BLOCK_LEVEL_PATH = "/scratch/michal/SourcererCC/SourcererCC/tokenizers/block-level/"
#PAIRS_PATH = "/scratch/michal/SourcererCC/ufsf_cpp80_result_data/results_cpp_80.pairs"
#DICTIONARY_SAVE_PATH = "/scratch/michal/sourcerercc_ufsf_cpp80_dictionaries/"

# BLOCK_LEVEL_PATH = "/home/k1462425/Documents/Research/SourcererCC/tokenizers/block-level/"
# PAIRS_PATH = "/home/k1462425/Documents/Research/SourcererCC/ufsf_java80_result_data_5/results.txt"
# DICTIONARY_SAVE_PATH = "/home/k1462425/Documents/Research/sourcerercc_ufsf_java80_dictionaries_5/"

# BLOCK_LEVEL_PATH = "/home/k1462425/Documents/Research/SourcererCC/result_ufsf_tok15_sim80_blockwise_all_cpp/"
# PAIRS_PATH = "/home/k1462425/Documents/Research/SourcererCC/result_ufsf_tok15_sim80_blockwise_all_cpp/results_tok15_sim80_ufsf_blockwise_all_cpp.txt"
# DICTIONARY_SAVE_PATH = "/home/k1462425/Documents/Research/SourcererCC/result_ufsf_tok15_sim80_blockwise_all_cpp/"
FULL_DATASET_PATH = "/home/k1462425/Documents/Research/malware-code-reuse-scripts/ufsf_java_repos_flat/ufsf_java_repos_flat/"
BLOCK_LEVEL_PATH = "/home/k1462425/Documents/Research/malware-code-reuse-scripts/SourcererCC-artifact/sourc_results/"
PAIRS_PATH = "/home/k1462425/Documents/Research/malware-code-reuse-scripts/SourcererCC-artifact/sourc_results/results.txt"
DICTIONARY_SAVE_PATH = BLOCK_LEVEL_PATH

# BLOCK_LEVEL_PATH = "/home/michael_tereszkowski/research/sourcerer/SourcererCC/result_ufsf_tok15_sim80_blockwise_all_cpp/"
# PAIRS_PATH = "/home/michael_tereszkowski/research/sourcerer/SourcererCC/result_ufsf_tok15_sim80_blockwise_all_cpp/results_tok15_sim80_ufsf_blockwise_all_cpp.txt"
# DICTIONARY_SAVE_PATH = "/home/michael_tereszkowski/research/sourcerer/SourcererCC/result_ufsf_tok15_sim80_blockwise_all_cpp/"

GLOBAL_FAILED = 0
SUCCESS = 0

def listdir_fullpath(d):
    return [os.path.join(d, f) for f in os.listdir(d)]

def get_projectids_blockids(all_lines):
    projectids_blockids = {}
    for line in all_lines:
        linesplit = line.split(",")
        project_id = linesplit[0][1:]     #get the id minus the 'b' or 'f' prefix
        firstcharacter = linesplit[0][0]
        if firstcharacter == 'f':
            if project_id not in projectids_blockids:
                projectids_blockids[project_id] = []
        if firstcharacter == 'b':
            block_id = linesplit[1]
            projectids_blockids[project_id].append(block_id)
    return projectids_blockids

def get_blockids_filepaths(all_lines):
    blockids_filepaths = {}
    fileid = 'ayooooooooooo'
    filepath = ''
    for line in all_lines:
        linesplit = line.split(",")
        firstcharacter = linesplit[0][0]
        if firstcharacter == 'f':
            fileid = linesplit[1]
            filepath = linesplit[2]
        if firstcharacter == 'b':
            blockid = linesplit[1]
            if blockid.endswith(fileid):
                blockids_filepaths[blockid] = filepath
    return blockids_filepaths

def get_projectids_numblocks(all_lines):
    projectids_numblocks = {}
    for line in all_lines:
        linesplit = line.split(",")
        project_id = linesplit[0][1:]
        if project_id not in projectids_numblocks:
            projectids_numblocks[project_id] = 0
        firstcharacter = linesplit[0][0]
        if firstcharacter == 'b':
            projectids_numblocks[project_id] = projectids_numblocks[project_id] + 1
    return projectids_numblocks

def get_num_overlap(lines_concerning,projectids_blockids,projectid):
    blocksOfProject = projectids_blockids[projectid]
    numblocks = len(blocksOfProject)
    blocks_cloned = set([])
    for line in lines_concerning:
        blocks_cloned.add(line.split(",")[1])
        blocks_cloned.add(line.split(",")[3])
    blocks_cloned = list(blocks_cloned)
    overlap = 0
    for block in blocksOfProject:
        if block in blocks_cloned:
            overlap = overlap + 1
    return overlap

def get_clonepercent(projectpair,projectids_blockids,results_pairs):
    lines_concerning = []
    for line in results_pairs:
        if len(line) > 0:
            linesplit = line.split(",")
            if (linesplit[0] == projectpair[0] and linesplit[2] == projectpair[1]) or (linesplit[2] == projectpair[0] and linesplit[0] == projectpair[1]):
                lines_concerning.append(line)

    num_overlap = get_num_overlap(lines_concerning,projectids_blockids,projectpair[0])
    total_blocks = len(projectids_blockids[projectpair[0]])
    clonepercent = num_overlap/total_blocks
    if projectpair == ('12786', '13362') or projectpair == ('13362','12786'):
        print(len(lines_concerning))
        print(lines_concerning)
        print(clonepercent)

    return clonepercent

def get_projectpairs_clonepercentage(projectids_numblocks,projectids_blockids):
    projectpairs_clonepercentage = {}
    lines = []
    with open(PAIRS_PATH,'r') as fp:
        lines = fp.read()
        lines = lines.split("\n")
    for line in lines:
        if len(line) > 0:
            linesplit = line.split(",")
            projectpairs_clonepercentage[(linesplit[0],linesplit[2])] = 0
            projectpairs_clonepercentage[(linesplit[2],linesplit[0])] = 0
    print("Number of project pairs: "+str(len(projectpairs_clonepercentage)))
    count = 0
    for projectpair in projectpairs_clonepercentage:
        host_clonepercent = get_clonepercent(projectpair,projectids_blockids,lines)
        projectpairs_clonepercentage[projectpair] = host_clonepercent
        count = count + 1
        if count % 100 == 0:
            print(count)
    return projectpairs_clonepercentage

def get_blockids_cloneblockids():
    blockids_cloneblockids = {}
    lines = []
    with open(PAIRS_PATH,'r') as fp:
        lines = fp.read()
        lines = lines.split("\n")
    for line in lines:
        if len(line) > 0:
            linesplit = line.split(",")
            hostblock = linesplit[1]
            cloneblock = linesplit[3]
            blockids_cloneblockids[hostblock] = []
            blockids_cloneblockids[cloneblock] = []
    for line in lines:
        if len(line) > 0:
            linesplit = line.split(",")
            hostblock = linesplit[1]
            cloneblock = linesplit[3]
            blockids_cloneblockids[hostblock].append(cloneblock)
            blockids_cloneblockids[cloneblock].append(hostblock)
    return blockids_cloneblockids


def get_blockids_cloneblockids_similarities(blockids_filepaths,blockids_cloneblockids,blockids_locations):
    blockids_cloneblockids_similarities = []
    num = 0
    for blockid in blockids_cloneblockids:
        for cloneblockid in blockids_cloneblockids[blockid]:
            num += 1
    print("Number of blocks to get through: "+str(num))

    count = 0
    for blockid in blockids_cloneblockids:
        for cloneblockid in blockids_cloneblockids[blockid]:
            triad = (blockid,cloneblockid,get_similarity_blockids(blockids_filepaths,blockid,cloneblockid,blockids_locations))
            blockids_cloneblockids_similarities.append(triad)
            count += 1
            if count % 10000 == 0:
                print(str(count)+"/"+str(num))

    return blockids_cloneblockids_similarities

def get_similarity_blockids(blockids_filepaths,blockid,cloneblockid,blockids_locations):
    path1 = blockids_filepaths[blockid]
    dir1 = path1.split("/")[-2]
    file1 = path1.split("/")[-1][:-1]
    path1 = FULL_DATASET_PATH+dir1+"/"+file1

    path2 = blockids_filepaths[cloneblockid]
    dir2 = path2.split("/")[-2]
    file2 = path2.split("/")[-1][:-1]
    path2 = FULL_DATASET_PATH+dir2+"/"+file2

    similarity = get_similarity((get_code(path1,blockids_locations[blockid][1]),get_code(path2,blockids_locations[cloneblockid][1])))
    return similarity

def get_similarity(pair):
    global GLOBAL_FAILED
    global SUCCESS
    code1 = pair[0]
    code2 = pair[1]

    ast_code1 = get_ast(code1)
    ast_code2 = get_ast(code2)

    if ast_code1 is None or len(ast_code1) == 0 or len(ast_code2) == 0 or ast_code2 is None:
        GLOBAL_FAILED += 1
        # print("FAILED!")
        # print(GLOBAL_FAILED)
        # print("SUCCESSES: "+str(SUCCESS))
        return 0.0

    # print(code1)

    # print(ast_code1)
    nodes_and_edges1 = ast_code1[1]
    hashes_and_tokens1 = ast_code1[2]
    # sys.exit(0)
    # print(code2)
    # print(ast_code2)
    nodes_and_edges2 = ast_code2[1]
    hashes_and_tokens2 = ast_code2[2]

    tree_similarity = get_tree_similarity(nodes_and_edges1,nodes_and_edges2,hashes_and_tokens1,hashes_and_tokens2)
    SUCCESS += 1
    return tree_similarity

def get_tree_similarity(nodes_and_edges1,nodes_and_edges2,hashes_and_tokens1,hashes_and_tokens2):
    wl_kernel = WeisfeilerLehman(base_graph_kernel=VertexHistogram,normalize=True)

    to_del = []
    for node in nodes_and_edges1:
        if len(nodes_and_edges1[node]) == 0:
            to_del.append(node)
    for node in to_del:
        del nodes_and_edges1[node]

    to_del = []
    for node in nodes_and_edges2:
        if len(nodes_and_edges2[node]) == 0:
            to_del.append(node)
    for node in to_del:
        del nodes_and_edges2[node]

    G1 = Graph(nodes_and_edges1, node_labels=hashes_and_tokens1)
    G2 = Graph(nodes_and_edges2, node_labels=hashes_and_tokens2)
    #wl_kernel.initialize()
    # print(nodes_and_edges1)
    # print("---------------------------------")
    # print(hashes_and_tokens1)
    # print(wl_kernel.fit_transform([G1]))
    # print(wl_kernel.transform([G2]))
    wl_kernel.fit_transform([G1])
    similarity = wl_kernel.transform([G2])
    #sys.exit(0)
    #print(wl_kernel.fit_transform([nodes_and_edges1]))
    #print(wl_kernel.transform([nodes_and_edges2]))
    return similarity

def get_ast(code):
    exceptionCount = 0
    members = []
    edges = {}
    hashes_and_tokens = {}
    try:
        tokens = list(javalang.tokenizer.tokenize(code))
        parser = javalang.parser.Parser(tokens)
        tree = parser.parse_member_declaration()
        #functions_apicalls[funcfile+":"+subfuncs[2][x]] = []
        #print(tree)

        members = get_sequence(tree,members,None,edges,hashes_and_tokens)

    except Exception as e:
        # print(e)
        exceptionCount += 1
        return members

    return members

def recurse_get_children(root,members):
    if root is None:
        return members

    if isinstance(root,javalang.tree.MethodInvocation):
        members.append(root)
    if isinstance(root,set) or isinstance(root,list):
        for node in root:
            members = recurse_get_children(node,members)
    elif not isinstance(root, str) and not isinstance(root,bool):
        children = root.children
        if children is not None:
            for child in children:
                if child is not None:
                    members = recurse_get_children(child,members)

    return members

def get_sequence(node, sequence, parentNodeHash, edges, hashes_and_tokens):
    token, children = get_token(node), get_children(node)
    sequence.append(token)
    nodeHash = produce_hash(token)
    hashes_and_tokens[nodeHash] = token
    if parentNodeHash is not None:
        if parentNodeHash not in edges:
            edges[parentNodeHash] = []
        else:
            edges[parentNodeHash].append(nodeHash)
    #print(sequence)

    for child in children:
        sequence,edges,hashes_and_tokens = get_sequence(child, sequence, nodeHash, edges, hashes_and_tokens)

    if token in ['ForStatement', 'WhileStatement', 'DoStatement','SwitchStatement', 'IfStatement']:
        sequence.append('End')

    return sequence,edges,hashes_and_tokens

def produce_hash(token):
    S = 10
    ran = ''.join(random.choices(string.ascii_uppercase + string.digits, k = S))
    return ran

def get_token(node):
    token = ''
    # if isinstance(node,javalang.tree.MethodDeclaration):
    #     print("METHOD DECLARATION!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
    if isinstance(node, str):
        token = node
    elif isinstance(node, set):
        token = 'Modifier'#node.pop()
    elif isinstance(node, Node):
        token = node.__class__.__name__

    return token

def get_children(root):
    if isinstance(root, Node):
        children = root.children
    elif isinstance(root, set):
        children = list(root)
    else:
        children = []

    def expand(nested_list):
        for item in nested_list:
            if isinstance(item, list):
                for sub_item in expand(item):
                    yield sub_item
            elif item:
                yield item

    return list(expand(children))

def get_code(path,line_locs):
    lines = []
    line = ""
    try:
        with open(path,'r') as fp:
            try:
                line = fp.readline()
                lines.append(line)
                while line:
                    line = fp.readline()
                    lines.append(line)
            except Exception as e:
                return None
    except Exception as e:
        return None

    lines_pertaining = lines[int(line_locs[0])-1:int(line_locs[1])]
    # code = "public class main {\n"
    # code = code + "".join(lines_pertaining)
    # code = code + "\n}"
    code = "".join(lines_pertaining) + "}"
    return code

def get_projectids_cloneprojectids():
    blockids_cloneblockids = {}
    lines = []
    with open(PAIRS_PATH,'r') as fp:
        lines = fp.read()
        lines = lines.split("\n")
    for line in lines:
        if len(line) > 0:
            linesplit = line.split(",")
            hostblock = linesplit[0]
            cloneblock = linesplit[2]
            blockids_cloneblockids[hostblock] = []
            blockids_cloneblockids[cloneblock] = []
    for line in lines:
        if len(line) > 0:
            linesplit = line.split(",")
            hostblock = linesplit[0]
            cloneblock = linesplit[2]
            blockids_cloneblockids[hostblock].append(cloneblock)
            blockids_cloneblockids[cloneblock].append(hostblock)
    return blockids_cloneblockids

def get_projectids_names():
    files = listdir_fullpath(BLOCK_LEVEL_PATH+"bookkeeping_projs/")
    projectids_names = {}
    all_lines = []
    for file in files:
        with open(file,'r') as fp:
            lines = fp.read()
            lines = lines.split("\n")
            for line in lines:
                all_lines.append(line)
    for line in all_lines:
        if len(line) > 0:
            linesplit = line.split(",")
            projectids_names[linesplit[0]] = linesplit[1].split("/")[-1].split(".zip")[0]
    return projectids_names

def get_blockids_frequencies():
    blockids_frequencies = {}
    lines = []
    with open(PAIRS_PATH,'r') as fp:
        lines = fp.read()
        lines = lines.split("\n")
    for line in lines:
        if len(line) > 0:
            linesplit = line.split(",")
            hostid = linesplit[1]
            cloneid = linesplit[3]
            if linesplit[0] != linesplit[2]:
                blockids_frequencies[hostid] = 0
                blockids_frequencies[cloneid] = 0
    for line in lines:
        if len(line) > 0:
            linesplit = line.split(",")
            hostid = linesplit[1]
            cloneid = linesplit[3]
            if linesplit[0] != linesplit[2]:
                blockids_frequencies[hostid] = blockids_frequencies[hostid] + 1
                blockids_frequencies[cloneid] = blockids_frequencies[cloneid] + 1
    return blockids_frequencies

def get_blockids_locations(blockids_frequencies,blockids_filepaths):
    files = listdir_fullpath(BLOCK_LEVEL_PATH+"blocks_stats")
    blockids_locations = {}
    all_lines = []
    for file in files:
        with open(file,'r') as fp:
            content = fp.read()
            lines = content.split("\n")
            for line in lines:
                if len(line) > 0:
                    all_lines.append(line)
    blockids_ranges = {}
    for line in all_lines:
        if len(line) > 5:
            linesplit = line.split(",")
            #print(linesplit)
            blockids_ranges[linesplit[1]] = (linesplit[-2],linesplit[-1])
    for blockid in blockids_filepaths:
        blockids_locations[blockid] = (blockids_filepaths[blockid],blockids_ranges[blockid])
    return blockids_locations

def main():
    global GLOBAL_FAILED
    global SUCCESS
    file_block_stats_path = BLOCK_LEVEL_PATH+"file_block_stats/"
    all_lines = []
    files = listdir_fullpath(file_block_stats_path)
    for file in files:
        print(file)
        with open(file,'r',encoding='latin1') as fp:
            contents = fp.read()
            contents_split = contents.split("\n")
            for line in contents_split:
                if len(line) != 0:
                    all_lines.append(line)

    projectids_blockids = get_projectids_blockids(all_lines)

    count = 0
    for projectid in projectids_blockids:
        if len(projectids_blockids[projectid]) != 0:
            count = count + 1
    print("Number of projects with at least one block: "+str(count))

    with open(DICTIONARY_SAVE_PATH+'projectids_blockids_java.pkl', 'wb') as handle:
        pickle.dump(projectids_blockids, handle, protocol=pickle.HIGHEST_PROTOCOL)

    blockids_filepaths = get_blockids_filepaths(all_lines)
    with open(DICTIONARY_SAVE_PATH+'blockids_filepaths_java.pkl', 'wb') as handle:
        pickle.dump(blockids_filepaths, handle, protocol=pickle.HIGHEST_PROTOCOL)

    projectids_numblocks = get_projectids_numblocks(all_lines)
    with open(DICTIONARY_SAVE_PATH+'projectids_numblocks_java.pkl', 'wb') as handle:
        pickle.dump(projectids_numblocks, handle, protocol=pickle.HIGHEST_PROTOCOL)

    blockids_cloneblockids = get_blockids_cloneblockids()
    count = 0
    for blockid in blockids_cloneblockids:
        if len(blockids_cloneblockids[blockid]) > 0:
            count += 1
    print("Number of blocks cloned: "+str(count))
    # sys.exit(0)
    with open(DICTIONARY_SAVE_PATH+'blockids_cloneblockids_java.pkl', 'wb') as handle:
        pickle.dump(blockids_cloneblockids, handle, protocol=pickle.HIGHEST_PROTOCOL)
    print("Saved the dict.")

    blockids_frequencies = get_blockids_frequencies()
    with open(DICTIONARY_SAVE_PATH+'blockids_frequencies_java.pkl', 'wb') as handle:
        pickle.dump(blockids_frequencies, handle, protocol=pickle.HIGHEST_PROTOCOL)

    blockids_locations = get_blockids_locations(blockids_frequencies,blockids_filepaths)
    with open(DICTIONARY_SAVE_PATH+'blockids_locations_java.pkl', 'wb') as handle:
        pickle.dump(blockids_locations, handle, protocol=pickle.HIGHEST_PROTOCOL)

    blockids_cloneblockids_similarities = get_blockids_cloneblockids_similarities(blockids_filepaths,blockids_cloneblockids,blockids_locations)
    df = pd.DataFrame(blockids_cloneblockids_similarities)
    df.columns = ['id1','id2','similarity']
    df.to_pickle(BLOCK_LEVEL_PATH+"blockids_cloneblockids_similarities.pkl")
    with open(DICTIONARY_SAVE_PATH+'blockids_cloneblockids_similarities.pkl', 'wb') as handle:
        pickle.dump(blockids_cloneblockids_similarities, handle, protocol=pickle.HIGHEST_PROTOCOL)
    #
    # print("FAILED!")
    # print(GLOBAL_FAILED)
    # print("SUCCESSES: "+str(SUCCESS))

    projectids_cloneprojectids = get_projectids_cloneprojectids()
    with open(DICTIONARY_SAVE_PATH+'projectids_cloneprojectids_java.pkl', 'wb') as handle:
        pickle.dump(projectids_cloneprojectids, handle, protocol=pickle.HIGHEST_PROTOCOL)
    print("Saved the dict.")

    projectids_names = get_projectids_names()
    with open(DICTIONARY_SAVE_PATH+'projectids_names_java.pkl', 'wb') as handle:
        pickle.dump(projectids_names, handle, protocol=pickle.HIGHEST_PROTOCOL)
    print("Saved the dict.")
    #print(projectids_names)

    projectpairs_clonepercentage = get_projectpairs_clonepercentage(projectids_numblocks,projectids_blockids)
    #print(projectpairs_clonepercentage)
    with open(DICTIONARY_SAVE_PATH+'projectpairs_clonepercentage_java.pkl', 'wb') as handle:
        pickle.dump(projectpairs_clonepercentage, handle, protocol=pickle.HIGHEST_PROTOCOL)


main()
