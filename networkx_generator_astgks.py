import networkx as nx
import pickle
import os
import pandas as pd
from datetime import datetime
from time import mktime
import sys
import statistics
import random
import Levenshtein
from collections import Counter

DICTIONARIES_PATH = "/home/k1462425/Documents/Research/malware-code-reuse-scripts/SourcererCC-artifact/sourc_results_ufsf/"
DICTIONARIES_PATH_50KC = "/home/k1462425/Documents/Research/malware-code-reuse-scripts/SourcererCC-artifact/sourc_results_50kc/"
DICTIONARIES_PATH_50KC_UFSF = "/home/k1462425/Documents/Research/malware-code-reuse-scripts/SourcererCC-artifact/sourc_results_hybrid/"
PRIORITISED_TYPES = ['spoof','ransomwar','backdoor','keylogg','rootkit','ddos']

attachments_paths_path = "projects-list-uc-attachments.txt"
pickles_path = ""
WHOLE_DATASET_PATH = "/home/k1462425/Documents/Research/malware-code-reuse-scripts/50KC_flat_with_cloned_postmethod_zips/"
UFSF_DATASET_PATH = "/home/k1462425/Documents/Research/malware-code-reuse-scripts/ufsf_java_repos_flat/ufsf_java_repos_flat/"
KC50_DATASET_PATH = "/home/k1462425/Documents/Research/malware-code-reuse-scripts/50KC_flat/"

blockids_cloneblockids_evaluation = []
counter = 0

def load_dictionary(path):
    b = {}
    with open(path, 'rb') as handle:
        b = pickle.load(handle)
    return b

def get_weight(projectid,cloneid,projectpairs_clonepercentage):
    weight1 = projectpairs_clonepercentage[(projectid,cloneid)]
    weight2 = projectpairs_clonepercentage[(cloneid,projectid)]
    theweight = 0
    if weight1 > weight2:
        theweight = weight1
    elif weight2 > weight1:
        theweight = weight2
    else:
        theweight = weight1
    return theweight

def main():
    # G = nx.Graph()
    G = nx.DiGraph()
    projectids_blockids = load_dictionary(DICTIONARIES_PATH+'projectids_blockids_java.pkl')
    blockids_filepaths = load_dictionary(DICTIONARIES_PATH+'blockids_filepaths_java.pkl')
    projectids_numblocks = load_dictionary(DICTIONARIES_PATH+'projectids_numblocks_java.pkl')
    projectpairs_clonepercentage = load_dictionary(DICTIONARIES_PATH+'projectpairs_clonepercentage_java.pkl')
    blockids_cloneblockids = load_dictionary(DICTIONARIES_PATH+'blockids_cloneblockids_java.pkl')
    projectids_cloneprojectids = load_dictionary(DICTIONARIES_PATH+'projectids_cloneprojectids_java.pkl')
    projectids_names = load_dictionary(DICTIONARIES_PATH+'projectids_names_java.pkl')
    repolinks_types_withmultilabel = load_dictionary(pickles_path+'repolinks_types_withmultilabel.pkl')
    blockids_cloneblockids_similarities = pd.read_pickle(DICTIONARIES_PATH+'blockids_cloneblockids_similarities.pkl')
    blockids_cloneblockids_similarities_filtered = pd.read_pickle(DICTIONARIES_PATH+'blockids_cloneblockids_similarities_filtered_java.pkl')
    blockids_locations = pd.read_pickle(DICTIONARIES_PATH+"blockids_locations_java.pkl")
    projectnames_ids = {}
    for projectid in projectids_names:
        projectnames_ids[projectids_names[projectid]] = projectid
    blockids_cloneblockids_similarities = pd.read_pickle(DICTIONARIES_PATH+'blockids_cloneblockids_similarities.pkl')
    projectids_blockids_50kc = load_dictionary(DICTIONARIES_PATH_50KC+'projectids_blockids_java.pkl')
    blockids_filepaths_50kc = load_dictionary(DICTIONARIES_PATH_50KC+'blockids_filepaths_java.pkl')
    projectids_numblocks_50kc = load_dictionary(DICTIONARIES_PATH_50KC+'projectids_numblocks_java.pkl')
    projectpairs_clonepercentage_50kc = load_dictionary(DICTIONARIES_PATH_50KC+'projectpairs_clonepercentage_java.pkl')
    blockids_cloneblockids_50kc = load_dictionary(DICTIONARIES_PATH_50KC+'blockids_cloneblockids_java.pkl')
    projectids_cloneprojectids_50kc = load_dictionary(DICTIONARIES_PATH_50KC+'projectids_cloneprojectids_java.pkl')
    projectids_names_50kc = load_dictionary(DICTIONARIES_PATH_50KC+'projectids_names_java.pkl')
    # repolinks_types_withmultilabel_50kc = load_dictionary(DICTIONARIES_PATH_50KC+'repolinks_types_withmultilabel.pkl')
    blockids_locations_50kc = pd.read_pickle(DICTIONARIES_PATH_50KC+"blockids_locations_java.pkl")

    projectids_blockids_50kc_ufsf = load_dictionary(DICTIONARIES_PATH_50KC_UFSF+'projectids_blockids_java.pkl')
    blockids_filepaths_50kc_ufsf = load_dictionary(DICTIONARIES_PATH_50KC_UFSF+'blockids_filepaths_java.pkl')
    projectids_numblocks_50kc_ufsf = load_dictionary(DICTIONARIES_PATH_50KC_UFSF+'projectids_numblocks_java.pkl')
    projectpairs_clonepercentage_50kc_ufsf = load_dictionary(DICTIONARIES_PATH_50KC_UFSF+'projectpairs_clonepercentage_java.pkl')
    blockids_cloneblockids_50kc_ufsf = load_dictionary(DICTIONARIES_PATH_50KC_UFSF+'blockids_cloneblockids_java.pkl')
    projectids_cloneprojectids_50kc_ufsf = load_dictionary(DICTIONARIES_PATH_50KC_UFSF+'projectids_cloneprojectids_java.pkl')
    projectids_names_50kc_ufsf = load_dictionary(DICTIONARIES_PATH_50KC_UFSF+'projectids_names_java.pkl')
    # repolinks_types_withmultilabel_50kc = load_dictionary(DICTIONARIES_PATH_50KC+'repolinks_types_withmultilabel.pkl')
    blockids_locations_50kc_ufsf = pd.read_pickle(DICTIONARIES_PATH_50KC_UFSF+"blockids_locations_java.pkl")

    projectnames_blockids = get_projectnames_blockids(projectids_blockids,projectids_names)
    blockids_projectnames = get_blockids_projectnames(projectnames_blockids)
    projectnames_blockids_50kc_ufsf = get_projectnames_blockids(projectids_blockids_50kc_ufsf,projectids_names_50kc_ufsf)
    blockids_projectnames_50kc_ufsf = get_blockids_projectnames(projectnames_blockids_50kc_ufsf)

    blockids_numclones = get_blockids_numclones(blockids_cloneblockids_similarities_filtered,blockids_projectnames)
    blockids_projectnames_appearing_in = get_blockids_projectnames_appearing_in(blockids_cloneblockids_similarities_filtered,blockids_projectnames)
    blockids_numprojects = get_blockids_numprojects(blockids_projectnames_appearing_in)
    blockids_most_popular = dict(Counter(blockids_numprojects).most_common(4000))


    blockids_cloneblockids_similarities_50kc = get_blockids_cloneblockids_similarities(blockids_filepaths_50kc,blockids_cloneblockids_50kc,blockids_locations_50kc)
    projectnames_blockids_50kc = get_projectnames_blockids(projectids_blockids_50kc,projectids_names_50kc)
    blockids_projectnames_50kc = get_blockids_projectnames(projectnames_blockids_50kc)
    blockids_numclones_50kc = get_blockids_numclones(blockids_cloneblockids_similarities_50kc,blockids_projectnames_50kc)
    blockids_projectnames_appearing_in_50kc = get_blockids_projectnames_appearing_in(blockids_cloneblockids_similarities_50kc,blockids_projectnames_50kc)
    blockids_numprojects_50kc = get_blockids_numprojects(blockids_projectnames_appearing_in_50kc)
    blockids_most_popular_50kc = dict(Counter(blockids_numprojects_50kc).most_common(4000))


    num_duplicates_found = 0
    x = 0
    duplicates = []
    for blockid in blockids_most_popular:
        found = False
        for blockid2 in blockids_most_popular_50kc:
            if code_equal(blockid,blockids_locations,blockid2,blockids_locations_50kc):
                found = True
                duplicates.append((blockid,blockid2))
                break
        if found:
            num_duplicates_found += 1
        x += 1
        print(str(x)+"/"+str(len(blockids_most_popular)))

    for pair in duplicates:
        blockid = pair[0]
        blockid2 = pair[1]
        print(blockid)
        print(blockids_numprojects[blockid])
        print(blockids_projectnames_appearing_in[blockid])
        print("----------------------------------")
        print(blockid2)
        print(blockids_numprojects_50kc[blockid2])
        print(blockids_projectnames_appearing_in_50kc[blockid2])
        print(get_code_by_location(blockids_locations[blockid]))
        print("++++++++++++++++++++++++++++++++++")
        print("++++++++++++++++++++++++++++++++++")
        print("++++++++++++++++++++++++++++++++++")

    print("Number of duplicates found: "+str(num_duplicates_found))

    import pickle
    # with open(DICTIONARIES_PATH+'50KC_and_postmethod_common_blockids.pkl', 'wb') as f:
    #     pickle.dump(duplicates, f)

    # with open(DICTIONARIES_PATH+'50KC_and_postmethod_common_blockids.pkl', 'rb') as f:
    #     duplicates = pickle.load(f)


    blockids_to_copy = set([])
    x = 0
    for triad in blockids_cloneblockids_similarities_filtered:
        blockids_to_copy.add(triad[0])
        blockids_to_copy.add(triad[1])
        x += 1
        print(str(x)+"/"+str(len(blockids_cloneblockids_similarities_filtered)))
    print(len(blockids_to_copy))
    blockids_to_copy = list(blockids_to_copy)
    # if os.path.exists(DICTIONARIES_PATH+'blockids_to_copy.pkl'):
    #     os.remove(DICTIONARIES_PATH+'blockids_to_copy.pkl')
    # with open(DICTIONARIES_PATH+'blockids_to_copy.pkl', 'wb') as f:
    #     pickle.dump(blockids_to_copy, f)

    # paths_and_blockids = inject_into_dataset(blockids_to_copy,blockids_locations)
    # if os.path.exists(DICTIONARIES_PATH+'paths_and_blockids.pkl'):
    #     os.remove(DICTIONARIES_PATH+'paths_and_blockids.pkl')
    # with open(DICTIONARIES_PATH+'paths_and_blockids.pkl', 'wb') as f:
    #     pickle.dump(paths_and_blockids, f)

    paths_and_blockids = {}
    with open(DICTIONARIES_PATH+'paths_and_blockids.pkl', 'rb') as f:
        paths_and_blockids = pickle.load(f)

    blockids_reused_in_benign,blockids_astgks_and_50kc = get_blockids_reused_in_benign(paths_and_blockids,blockids_cloneblockids_50kc_ufsf,blockids_locations_50kc_ufsf)
    # blockids_to_copy_unique = get_blockids_to_copy_unique(blockids_to_copy,blockids_locations)
    # import pickle
    # with open(DICTIONARIES_PATH+'blockids_to_copy_unique.pkl', 'wb') as f:
    #     pickle.dump(blockids_to_copy_unique, f)

    # with open(DICTIONARIES_PATH+'50KC_and_postmethod_common_blockids.pkl', 'rb') as f:
    #     duplicates = pickle.load(f)
    blockids = []
    for blockid in blockids_cloneblockids:
        blockids.append(blockid)
        for cloneblockid in blockids_cloneblockids[blockid]:
            blockids.append(cloneblockid)
    blockids = list(set(blockids))
    print("Number of blocks total: "+str(len(blockids)))
    print(len(blockids_reused_in_benign))

    blockids_reused_in_benign = discard_blockids_same_repos(blockids_reused_in_benign,blockids_locations,blockids_locations_50kc_ufsf,blockids_astgks_and_50kc,blockids_projectnames,blockids_projectnames_50kc_ufsf)
    print(len(blockids_reused_in_benign))
    # sys.exit(0)

    blockids_cloneblockids_similarities_filtered = takeout(duplicates,blockids_cloneblockids_similarities_filtered)
    print(len(blockids_cloneblockids_similarities_filtered))
    blockids_cloneblockids_similarities_filtered = takeout_1d(blockids_reused_in_benign,blockids_cloneblockids_similarities_filtered)
    print(len(blockids_cloneblockids_similarities_filtered))
    # sys.exit(0)
    blockids_projectids = {}
    for projectid in projectids_blockids:
        for blockid in projectids_blockids[projectid]:
            blockids_projectids[blockid] = projectid

    projectpairs_edgeweights = {}
    count = 0
    for triad in blockids_cloneblockids_similarities_filtered:
        projectpairs_edgeweights[(blockids_projectids[triad[0]],blockids_projectids[triad[1]])] = float(0.0)
    for triad in blockids_cloneblockids_similarities_filtered:
        if projectpairs_edgeweights[(blockids_projectids[triad[0]],blockids_projectids[triad[1]])] < float(triad[2]):
            projectpairs_edgeweights[(blockids_projectids[triad[0]],blockids_projectids[triad[1]])] = float(triad[2])
        count += 1
        if count % 10000 == 0:
            print(str(count)+"/"+str(len(blockids_cloneblockids_similarities_filtered)))

    print(projectpairs_edgeweights)
    print(blockids_cloneblockids_similarities_filtered)
    # sys.exit(0)
    import pickle
    if not os.path.exists(DICTIONARIES_PATH+"projectpairs_edgeweights_above07_normalized.pkl"):
        projectpairs_edgeweights_above07_normalized = truncate_and_normalize(projectpairs_edgeweights)
        with open(DICTIONARIES_PATH+"projectpairs_edgeweights_above07_normalized.pkl", 'wb') as handle:
            pickle.dump(projectpairs_edgeweights_above07_normalized, handle, protocol=pickle.HIGHEST_PROTOCOL)
    else:
        with open(DICTIONARIES_PATH+"projectpairs_edgeweights_above07_normalized.pkl", 'rb') as handle:
            projectpairs_edgeweights_above07_normalized = pickle.load(handle)

    import pickle5 as pickle
    with open("repolinks_and_directories.pkl", "rb") as fh:
        repolinks_and_directories = pickle.load(fh)

    projectpairs_edgeweights = projectpairs_edgeweights_above07_normalized
    # print(projectpairs_edgeweights)
    # sys.exit(0)

    attachments_paths = []
    with open(attachments_paths_path,'r') as fp:
        line = fp.readline()
        if len(line) > 3:
            attachments_paths.append(line)
        while line:
            line = fp.readline()
            if len(line) > 3:
                attachments_paths.append(line)

    for x in range(len(attachments_paths)):
        attachments_paths[x] = attachments_paths[x].split("/")[-1].split(".zip")[0]


    nodenames_malwaretypes = {}
    for projectid in projectids_cloneprojectids:
        nodenames = projectids_cloneprojectids[projectid]
        for node in nodenames:
            nodename = projectids_names[node]
            nodenames_malwaretypes[nodename] = ""

    # print(attachments_paths)
    print("----------------------------------------")
    print(len(nodenames_malwaretypes))

    found = False
    directories_and_repolinks = {value:key for key, value in repolinks_and_directories.items()}
    for nodename in nodenames_malwaretypes:
        if nodename in attachments_paths:
            found = True
            nodenames_malwaretypes[nodename] = ['attachment']
        for directory in directories_and_repolinks:
            dirname = directory.split("/")[-1]
            if dirname == nodename:
                repolink = directories_and_repolinks[directory]
                if repolink in repolinks_types_withmultilabel.keys():
                    nodenames_malwaretypes[nodename] = repolinks_types_withmultilabel[repolink]
                else:
                    nodenames_malwaretypes[nodename] = ["unknown"]

    print(nodenames_malwaretypes)

    print(found)

    for nodename in nodenames_malwaretypes:
        if len(nodenames_malwaretypes[nodename]) == 0:
            nodenames_malwaretypes[nodename] = ['attachment']

    for projectid in projectids_cloneprojectids:
        nodenames = projectids_cloneprojectids[projectid]
        for node in nodenames:
            G.add_node(node,label=projectids_names[node],numblocks=projectids_numblocks[node],malwaretype=nodenames_malwaretypes[projectids_names[node]][0])

    repos_and_timemins = {}
    with open(pickles_path+'repo_and_timemins.pkl','rb') as fp:
        repos_and_timemins = pickle.load(fp)
    repodirs_and_timemins = get_repodirs_and_timespans(repos_and_timemins)

    print(projectids_names)
    # sys.exit(0)
    edges = []
    edges_projectnames = []
    repos_connected = {}
    all_projectnames = []
    the_edge_weights = []
    print("PROJECTPAIRS_EDGEWEIGHTS:")
    print(projectpairs_edgeweights)
    print("PROJECTPAIRS_EDGEWEIGHS^")
    for projectid in projectids_cloneprojectids:
        # print("PROJECTID")
        # print(projectid)
        for cloneid in projectids_cloneprojectids[projectid]:
            # print("CLONEID")
            # print(cloneid)
            if projectid != cloneid:
                # print("NOT SAME")
                if (projectid,cloneid) in projectpairs_edgeweights or (cloneid,projectid) in projectpairs_edgeweights:
                    print("IN EDGEWEIGHTS")
                    # weight = get_weight(projectid,cloneid,projectpairs_clonepercentage)
                    weight = get_weight(projectid,cloneid,projectpairs_edgeweights)
                    if weight == 0:
                        weight = 0.01
                    projectname1 = projectids_names[projectid]
                    projectname2 = projectids_names[cloneid]
                    all_projectnames.append(projectname1)
                    all_projectnames.append(projectname2)
                    timemin1 = repodirs_and_timemins[projectname1]
                    timemin2 = repodirs_and_timemins[projectname2]
                    the_edge_weights.append(weight)
                    if timemin2 < timemin1:
                        G.add_edge(cloneid,projectid,weight=weight)
                    elif timemin1 < timemin2:
                        G.add_edge(projectid,cloneid,weight=weight)
                    else:
                        G.add_edge(projectid,cloneid,weight=weight)
                        G.add_edge(cloneid,projectid,weight=weight)
                    G.add_edge(projectid,cloneid,weight=weight)
                    edges.append((projectid,cloneid))
                    edges_projectnames.append((projectids_names[projectid],projectids_names[cloneid]))
                    if projectids_names[projectid] not in repos_connected.keys():
                        repos_connected[projectids_names[projectid]] = []
                    else:
                        repos_connected[projectids_names[projectid]].append(projectids_names[cloneid])
                    if projectids_names[cloneid] not in repos_connected.keys():
                        repos_connected[projectids_names[cloneid]] = []
                    else:
                        repos_connected[projectids_names[cloneid]].append(projectids_names[projectid])
                    # G.add_edge(projectid,cloneid,weight=weight)
                # G.add_edge(projectid,cloneid)

    all_projectnames = list(set(all_projectnames))

    blockids_numclones = {}
    blockids_projectnames_appearing_in = {}
    blockids_numprojects = {}
    blockids_most_popular = {}
    import pickle
    if not os.path.exists(DICTIONARIES_PATH+"blockids_numclones.pkl"):
        blockids_numclones = get_blockids_numclones(blockids_cloneblockids_similarities_filtered,blockids_projectnames)
        with open(DICTIONARIES_PATH+"blockids_numclones.pkl", 'wb') as handle:
            pickle.dump(blockids_numclones, handle, protocol=pickle.HIGHEST_PROTOCOL)
    else:
        with open(DICTIONARIES_PATH+"blockids_numclones.pkl", 'rb') as handle:
            blockids_numclones = pickle.load(handle)
    if not os.path.exists(DICTIONARIES_PATH+"blockids_projectnames_appearing_in.pkl"):
        blockids_projectnames_appearing_in = get_blockids_projectnames_appearing_in(blockids_cloneblockids_similarities_filtered,blockids_projectnames)
        with open(DICTIONARIES_PATH+"blockids_projectnames_appearing_in.pkl", 'wb') as handle:
            pickle.dump(blockids_projectnames_appearing_in, handle, protocol=pickle.HIGHEST_PROTOCOL)
    else:
        with open(DICTIONARIES_PATH+"blockids_projectnames_appearing_in.pkl", 'rb') as handle:
            blockids_projectnames_appearing_in = pickle.load(handle)

    if not os.path.exists(DICTIONARIES_PATH+"blockids_numprojects.pkl"):
        blockids_numprojects = get_blockids_numprojects(blockids_projectnames_appearing_in)
        with open(DICTIONARIES_PATH+"blockids_numprojects.pkl", 'wb') as handle:
            pickle.dump(blockids_numprojects, handle, protocol=pickle.HIGHEST_PROTOCOL)
    else:
        with open(DICTIONARIES_PATH+"blockids_numprojects.pkl", 'rb') as handle:
            blockids_numprojects = pickle.load(handle)

    if not os.path.exists(DICTIONARIES_PATH+"blockids_most_popular.pkl"):
        blockids_most_popular = dict(Counter(blockids_numprojects).most_common(4000))
        with open(DICTIONARIES_PATH+"blockids_most_popular.pkl", 'wb') as handle:
            pickle.dump(blockids_most_popular, handle, protocol=pickle.HIGHEST_PROTOCOL)
    else:
        with open(DICTIONARIES_PATH+"blockids_most_popular.pkl", 'rb') as handle:
            blockids_most_popular = pickle.load(handle)

    for blockid in blockids_most_popular:
        print(blockids_locations[blockid])

    print("_-------------------------------------------")
    blockids_most_popular_across_projects = dict(Counter(blockids_numprojects).most_common(len(blockids_numprojects)))

    pr = nx.pagerank(G)
    print(pr)
    sizes = []
    centralities = []
    in_centralities = []
    all_projects = []
    for projectid in projectids_cloneprojectids:
        for cloneid in projectids_cloneprojectids[projectid]:
            if projectid != cloneid:
                if (projectid,cloneid) in projectpairs_edgeweights or (cloneid,projectid) in projectpairs_edgeweights:
                    # all_projects.append(projectid)
                    all_projects.append(projectids_names[projectid])
                    all_projects.append(projectids_names[cloneid])
    all_projects = list(set(all_projects))
    repodirs_and_centralities = {}
    repodirs_and_in_centralities = {}
    import numpy
    # degree_centrality = nx.degree_centrality(G)
    # katz_centrality = nx.katz_centrality_numpy(G, weight= 'weight')
    # degree_centrality = nx.degree_centrality(G)
    out_degree_centrality = nx.out_degree_centrality(G)
    in_degree_centrality = nx.in_degree_centrality(G)
    # subgraph = G.subgraph(Gcc[9])
    for project in all_projects:
        sizes.append(projectids_numblocks[projectnames_ids[project]])
        # centralities.append(pr[projectnames_ids[project]])
        # centralities.append(katz_centrality[projectnames_ids[project]])
        centralities.append(out_degree_centrality[projectnames_ids[project]])
        in_centralities.append(in_degree_centrality[projectnames_ids[project]])
        # repodirs_and_centralities[project] = pr[projectnames_ids[project]]
        # subgraph = G.subgraph(Gcc[0])
        # nodes = nx.nodes(subgraph)
        # nodes = nx.nodes(G)
        # subgraph1 = G.subgraph(Gcc[1])
        # nodes1 = nx.nodes(subgraph1)
        # print(nodes)
        # print(len(nodes))
        # print(len(nodes1))
        # projectnames_in_subgraph = []
        # for node in nodes:
        #     projectnames_in_subgraph.append(projectids_names[node])
        # sys.exit(0)
        # if project in projectnames_in_subgraph:
            # repodirs_and_centralities[project] = katz_centrality[projectnames_ids[project]]
        repodirs_and_centralities[project] = out_degree_centrality[projectnames_ids[project]]
        repodirs_and_in_centralities[project] = in_degree_centrality[projectnames_ids[project]]

    print(all_projects)
    print(repodirs_and_centralities)
    with open(DICTIONARIES_PATH+'repodirs_and_centralities_outdeg.pkl', 'wb') as f:
        pickle.dump(repodirs_and_centralities, f)

    with open(DICTIONARIES_PATH+'repodirs_and_centralities_indeg.pkl', 'wb') as f:
        pickle.dump(repodirs_and_in_centralities, f)

    # repolinks_postmethod = get_repolinks_postmethod(directories_and_repolinks,all_projects)
    # with open(DICTIONARIES_PATH+'repolinks_postmethod.pkl', 'wb') as f:
    #     pickle.dump(repolinks_postmethod, f)

    biggest_pr_nodes = dict(Counter(pr).most_common(50))

    if os.path.exists(DICTIONARIES_PATH+"projects_java40_filtered_50kcfiltered_directed.gexf"):
        os.remove(DICTIONARIES_PATH+"projects_java40_filtered_50kcfiltered_directed.gexf")

    nx.write_gexf(G, DICTIONARIES_PATH+"projects_java40_filtered_50kcfiltered_directed.gexf")



def get_repolinks_postmethod(directories_and_repolinks,all_projects):
    repolinks = []
    dirnames_and_repolinks = {}
    for directory in directories_and_repolinks:
        dirnames_and_repolinks[directory.split("/")[-1]] = directories_and_repolinks[directory]
    for project in all_projects:
        repolinks.append(dirnames_and_repolinks[project])
    repolinks = list(set(repolinks))
    return repolinks

def takeout_1d(blockids_reused_in_benign,blockids_cloneblockids_similarities_filtered):
    blockids_cloneblockids_similarities_filtered2 = []
    to_remove = []
    for triad in blockids_cloneblockids_similarities_filtered:
        if not (triad[0] in blockids_reused_in_benign) and not (triad[1] in blockids_reused_in_benign):
            blockids_cloneblockids_similarities_filtered2.append(triad)
    return blockids_cloneblockids_similarities_filtered2


def discard_blockids_same_repos(blockids_reused_in_benign,blockids_locations,blockids_locations_50kc_ufsf,blockids_astgks_and_50kc,blockids_projectnames,blockids_projectnames_50kc_ufsf):
    blockids_50kc_and_astgks = {}
    blockids_to_remove = []
    projectnames_in_common = []
    for blockid in blockids_astgks_and_50kc:
        blockids_50kc_and_astgks[blockids_astgks_and_50kc[blockid]] = blockid
    for blockid in blockids_reused_in_benign:
        projectname_astgks = blockids_projectnames[blockid]
        blockid_50kc = blockids_astgks_and_50kc[blockid]
        projectname_50kc = blockids_projectnames_50kc_ufsf[blockid_50kc]
        if projectname_astgks == projectname_50kc:
            blockids_to_remove.append(blockid)
            projectnames_in_common.append(projectname_astgks)

    projectnames_in_common = list(set(projectnames_in_common))
    print("Number of projects in common: "+str(len(projectnames_in_common)))
    print(projectnames_in_common)

    for blockid in blockids_to_remove:
        blockids_reused_in_benign.remove(blockid)
    return blockids_reused_in_benign


def get_blockids_reused_in_benign(paths_and_blockids,blockids_cloneblockids_50kc_ufsf,blockids_locations_50kc_ufsf):
    blockids_astgks_and_50kc = {}
    blockids_reused_in_benign = []
    for blockid in blockids_cloneblockids_50kc_ufsf:
        for cloneblockid in blockids_cloneblockids_50kc_ufsf[blockid]:
            astgks_blockid = get_astgks_blockid(blockid,paths_and_blockids,blockids_locations_50kc_ufsf)
            astgks_cloneblockid = get_astgks_blockid(blockid,paths_and_blockids,blockids_locations_50kc_ufsf)
            if astgks_blockid != 0:
                blockids_reused_in_benign.append(astgks_blockid)
                blockids_astgks_and_50kc[astgks_blockid] = blockid
            if astgks_cloneblockid != 0:
                blockids_reused_in_benign.append(astgks_cloneblockid)
                blockids_astgks_and_50kc[astgks_cloneblockid] = cloneblockid
    blockids_reused_in_benign = list(set(blockids_reused_in_benign))

    return blockids_reused_in_benign,blockids_astgks_and_50kc

def get_astgks_blockid(blockid,paths_and_blockids,blockids_locations_50kc_ufsf):
    location = blockids_locations_50kc_ufsf[blockid]
    # print(paths_and_blockids)
    file = ""
    # print(location[0])
    # sys.exit(0)
    if "astgks" in location[0]:
        # print("done")
        # sys.exit(0)
        file = location[0]
        dir = file.split("/")[-2]
        filename = file.split("/")[-1]
        file = WHOLE_DATASET_PATH+dir+"/"+filename
        file = file[:-1]

    astgks_blockid = 0
    if len(file) > 0:
        # print(file)
        # sys.exit(0)
        astgks_blockid = paths_and_blockids[file]
    return astgks_blockid

def inject_into_dataset(blockids_to_copy,blockids_locations):
    paths_and_blockids = {}
    destination_path = WHOLE_DATASET_PATH
    if not os.path.exists(destination_path):
        os.mkdir(destination_path)
    x = 0
    for blockid in blockids_to_copy:
        with open(destination_path+"sample"+str(x)+".java",'a+') as fp:
            fp.write("public class Dummy {\n")
            fp.write(get_code_by_location(blockids_locations[blockid]))
            fp.write("\n}")
            paths_and_blockids[destination_path+"sample"+str(x)+".java"] = blockid
        print("Wrote to: "+destination_path+"sample"+str(x)+".java "+str(x))
        x += 1
    return paths_and_blockids

def get_unique_dups(duplicates,blockids_locations):
    unique_dups = []
    for x in range(len(duplicates)):
        found = False
        dup1 = duplicates[x][0]
        for y in range(len(unique_dups)):
            dup2 = unique_dups[y]
            code1 = get_code_by_location(blockids_locations[dup1])
            code2 = get_code_by_location(blockids_locations[dup2])
            distance = Levenshtein.distance(code1,code2)
            if distance < 10:
                found = True
        if not found:
            unique_dups.append(dup1)

    unique_dups = list(set(unique_dups))

    return unique_dups

def get_blockids_to_copy_unique(blockids_to_copy,blockids_locations):
    duplicates = blockids_to_copy
    unique_dups = []
    x = 0
    for x in range(len(duplicates)):
        found = False
        dup1 = duplicates[x]
        for y in range(len(unique_dups)):
            dup2 = unique_dups[y]
            code1 = get_code_by_location(blockids_locations[dup1])
            code2 = get_code_by_location(blockids_locations[dup2])
            # distance = Levenshtein.distance(code1,code2)
            # if distance < 10:
            if code1 == code2:
                found = True
        if not found:
            unique_dups.append(dup1)
        x += 1
        if x % 100 == 0:
            print(str(x)+"/"+str(len(duplicates)))
            print("Length of unique_dups: "+str(len(unique_dups)))

    unique_dups = list(set(unique_dups))

    return unique_dups


def get_unique_funcs_blockids(blockids_cloneblockids_similarities_filtered,blockids_locations):
    unique_funcs_blockids = []
    unique_dups = []
    blockids_done = []
    for x in range(len(blockids_cloneblockids_similarities_filtered)):
        found = False
        dup1 = blockids_cloneblockids_similarities_filtered[x][0]
        for y in range(len(unique_dups)):
            dup2 = unique_dups[y]
            code1 = get_code_by_location(blockids_locations[dup1])
            code2 = get_code_by_location(blockids_locations[dup2])
            # distance = Levenshtein.distance(code1,code2)
            # if distance < 10:
            if code1 == code2:
                found = True
        if not found:
            unique_dups.append(dup1)
        if x % 100 == 0:
            print(str(x)+"/"+str(len(blockids_cloneblockids_similarities_filtered)))
            print("Length of unique_dups: "+str(len(unique_dups)))
        blockids_done.append(dup1)

    for x in range(len(blockids_cloneblockids_similarities_filtered)):
        found = False
        dup1 = blockids_cloneblockids_similarities_filtered[x][1]
        if dup1 not in blockids_done:
            for y in range(len(unique_dups)):
                dup2 = unique_dups[y]
                code1 = get_code_by_location(blockids_locations[dup1])
                code2 = get_code_by_location(blockids_locations[dup2])
                # distance = Levenshtein.distance(code1,code2)
                # if distance < 10:
                if code1 == code2:
                    found = True
            if not found:
                unique_dups.append(dup1)
        if x % 100 == 0:
            print(str(x)+"/"+str(len(blockids_cloneblockids_similarities_filtered)))
            print("Length of unique_dups: "+str(len(unique_dups)))
        blockids_done.append(dup1)

    unique_dups = list(set(unique_dups))
    return unique_dups


def takeout(duplicates,blockids_cloneblockids_similarities_filtered):
    triads_to_remove = []
    x = 0
    for duplicate in duplicates:
        for triad in blockids_cloneblockids_similarities_filtered:
            if (duplicate[0] == triad[0] or duplicate[0] == triad[1]):
                triads_to_remove.append(triad)
        x += 1
        print(str(x)+"/"+str(len(duplicates)))
    x = 0
    for triad in triads_to_remove:
        try:
            blockids_cloneblockids_similarities_filtered.remove(triad)
        except Exception as e:
            print(e)
        x += 1
        print(str(x)+"/"+str(len(triads_to_remove)))
    return blockids_cloneblockids_similarities_filtered

def code_equal(blockid,blockids_locations,blockid2,blockids_locations_50kc):
    code1 = get_code_by_location(blockids_locations[blockid])
    code2 = get_code_by_location_50kc(blockids_locations_50kc[blockid2])
    # print(code1)
    # print("------------------------------------")
    # print(code2)
    # print("++++++++++++++++++++++++++++++++++++")
    # print("++++++++++++++++++++++++++++++++++++")
    # print("++++++++++++++++++++++++++++++++++++")
    distance = Levenshtein.distance(code1,code2)
    if distance < 10:
        return True
    return False

def get_blockids_cloneblockids_similarities(blockids_filepaths,blockids_cloneblockids,blockids_locations):
    blockids_cloneblockids_similarities = []
    num = 0
    for blockid in blockids_cloneblockids:
        for cloneblockid in blockids_cloneblockids[blockid]:
            num += 1
    print("Number of blocks to get through: "+str(num))

    count = 0
    for blockid in blockids_cloneblockids:
        for cloneblockid in blockids_cloneblockids[blockid]:
            triad = (blockid,cloneblockid,1)
            blockids_cloneblockids_similarities.append(triad)
            count += 1
            if count % 10000 == 0:
                print(str(count)+"/"+str(num))

    return blockids_cloneblockids_similarities

def get_malware_types_links(edges,projectids_names,nodenames_malwaretypes):
    malware_types_links = {}
    for edgepair in edges:
        projectid = edgepair[0]
        cloneprojectid = edgepair[1]
        malware_types1 = nodenames_malwaretypes[projectids_names[projectid]]
        malware_types2 = nodenames_malwaretypes[projectids_names[cloneprojectid]]
        for mtype in malware_types1:
            for mtype2 in malware_types2:
                if (mtype,mtype2) in malware_types_links.keys():
                    malware_types_links[(mtype,mtype2)] += 1
                else:
                    malware_types_links[(mtype,mtype2)] = 0

                if (mtype2,mtype) in malware_types_links.keys():
                    malware_types_links[(mtype2,mtype)] += 1
                else:
                    malware_types_links[(mtype2,mtype)] = 0

    malware_types_links = trim_malware_types_links(malware_types_links)
    return malware_types_links

def trim_malware_types_links(malware_types_links):
    to_remove = []
    for elem in malware_types_links:
        if elem[0] == elem[1]:
            to_remove.append(elem)
        if elem[0] == 'unknown' or elem[1] == 'unknown':
            to_remove.append(elem)
    to_remove = list(set(to_remove))
    for toremove in to_remove:
        del malware_types_links[toremove]
    return malware_types_links


def get_code_of_projectid_pair(projectid,cloneprojectid,projectids_blockids,blockids_filepaths,blockids_locations,blockids_cloneblockids,blockids_projectids,blockids_cloneblockids_similarities_filtered):
    global blockids_cloneblockids_evaluation
    global counter
    blockid = -1
    blockids = projectids_blockids[projectid]
    blockids_filtered = get_blockids_filtered(blockids_cloneblockids_similarities_filtered)
    for blockid in blockids:
        if blockid in blockids_filtered:
            if blockid in blockids_cloneblockids.keys():
                cloneblockids = blockids_cloneblockids[blockid]
                for cloneblockid in cloneblockids:
                    if cloneprojectid in blockids_projectids[cloneblockid]:
                        # print_code(blockid,blockids_filepaths,blockids_locations)
                        code = get_code_to_print(blockid,blockids_filepaths,blockids_locations)
                        blockids_cloneblockids_evaluation.append((blockid,cloneblockid))
                        # print("+++++++++++++++++++++++++++")
                        # print("+++++++++++++++++++++++++++")
                        # print("+++++++++++++++++++++++++++")
                        counter += 1
                        # print(counter)
                        return code

def get_blockids_filtered(blockids_cloneblockids_similarities_filtered):
    blockids_filtered = []
    for triad in blockids_cloneblockids_similarities_filtered:
        blockids_filtered.append(triad[0])
        blockids_filtered.append(triad[1])
    blockids_filtered = list(set(blockids_filtered))
    return blockids_filtered

def print_code(blockid,blockids_filepaths,blockids_locations):
    path1 = blockids_filepaths[blockid]
    dir1 = path1.split("/")[-2]
    file1 = path1.split("/")[-1][:-1]
    path1 = "/media/k1462425/BackupsDrive/ufsf_java_repos_flat/ufsf_java_repos_flat/"+dir1+"/"+file1
    print(get_code(path1,blockids_locations[blockid][1]))

def get_code_to_print(blockid,blockids_filepaths,blockids_locations):
    path1 = blockids_filepaths[blockid]
    dir1 = path1.split("/")[-2]
    file1 = path1.split("/")[-1][:-1]
    path1 = "/media/k1462425/BackupsDrive/ufsf_java_repos_flat/ufsf_java_repos_flat/"+dir1+"/"+file1
    return get_code(path1,blockids_locations[blockid][1])


def get_code(path,line_locs):
    lines = []
    line = ""
    try:
        with open(path,'r') as fp:
            try:
                line = fp.readline()
                lines.append(line)
                while line:
                    line = fp.readline()
                    lines.append(line)
            except Exception as e:
                return None
    except Exception as e:
        return None

    lines_pertaining = lines[int(line_locs[0])-1:int(line_locs[1])]
    # code = "public class main {\n"
    # code = code + "".join(lines_pertaining)
    # code = code + "\n}"
    code = "".join(lines_pertaining) + "}"
    return code

def get_blockids_numprojects(blockids_projectnames_appearing_in):
    blockids_numprojects = {}
    for blockid in blockids_projectnames_appearing_in:
        blockids_numprojects[blockid] = len(blockids_projectnames_appearing_in[blockid])
    return blockids_numprojects

def get_blockids_projectnames_appearing_in(blockids_cloneblockids_similarities,blockids_projectnames):
    blockids_projectnames_appearing_in = {}
    for triad in blockids_cloneblockids_similarities:
        if float(triad[2]) >= 0.87:
            blockids_projectnames_appearing_in[triad[0]] = set([])
            blockids_projectnames_appearing_in[triad[1]] = set([])
    for triad in blockids_cloneblockids_similarities:
        if float(triad[2]) >= 0.87:
            blockids_projectnames_appearing_in[triad[0]].add(blockids_projectnames[triad[0]])
            blockids_projectnames_appearing_in[triad[0]].add(blockids_projectnames[triad[1]])
            blockids_projectnames_appearing_in[triad[1]].add(blockids_projectnames[triad[1]])
            blockids_projectnames_appearing_in[triad[1]].add(blockids_projectnames[triad[0]])
    return blockids_projectnames_appearing_in

def get_blockids_projectnames(projectnames_blockids):
    blockids_projectnames = {}
    for projectname in projectnames_blockids:
        blockids = projectnames_blockids[projectname]
        for blockid in blockids:
            blockids_projectnames[blockid] = projectname
    return blockids_projectnames

def get_projectnames_blockids(projectids_blockids,projectids_names):
    projectnames_blockids = {}
    for projectid in projectids_blockids:
        projectnames_blockids[projectids_names[projectid]] = projectids_blockids[projectid]
    return projectnames_blockids

def get_blockids_numclones(blockids_cloneblockids_similarities,blockids_projectnames):
    blockids_numclones = {}
    for triad in blockids_cloneblockids_similarities:
        if blockids_projectnames[triad[0]] != blockids_projectnames[triad[1]]:
            if float(triad[2]) >= 0.87:
                blockids_numclones[triad[0]] = 0
                blockids_numclones[triad[1]] = 0
    for triad in blockids_cloneblockids_similarities:
        if blockids_projectnames[triad[0]] != blockids_projectnames[triad[1]]:
            if float(triad[2]) >= 0.87:
                blockids_numclones[triad[0]] += 1
                blockids_numclones[triad[1]] += 1
    return blockids_numclones

def get_most_cloned_blockids_thresholded(blockids_cloneblockids_similarities):
    for y in range(len(blockids_cloneblockids_similarities)):
        for x in range(len(blockids_cloneblockids_similarities))-1:
            if float(blockids_cloneblockids_similarities[x][2]) < float(blockids_cloneblockids_similarities[x+1][2]):
                temp = blockids_cloneblockids_similarities[x+1]
                blockids_cloneblockids_similarities[x+1] = blockids_cloneblockids_similarities[x]
                blockids_cloneblockids_similarities[x] = temp
    return blockids_cloneblockids_similarities

def get_code_by_location(location):
    path = location[0][1:-1]
    suffix = path.split("/")[-2:]
    suffix = "/".join(suffix)
    prefix = UFSF_DATASET_PATH
    filepath = prefix + suffix
    lines = []
    with open(filepath,'r') as fp:
        line = fp.readline()
        lines.append(line)
        while line:
            line = fp.readline()
            lines.append(line)

    selected_lines = lines[int(location[1][0])-1:int(location[1][1])]
    code = "".join(selected_lines)
    return code

def get_code_by_location_50kc(location):
    path = location[0][1:-1]
    suffix = path.split("/")[-2:]
    suffix = "/".join(suffix)
    prefix = KC50_DATASET_PATH
    filepath = prefix + suffix
    lines = []
    with open(filepath,'r',encoding='latin1') as fp:
        line = fp.readline()
        lines.append(line)
        while line:
            line = fp.readline()
            lines.append(line)

    selected_lines = lines[int(location[1][0])-1:int(location[1][1])]
    code = "".join(selected_lines)
    return code


def get_code_by_location_50kc_ufsf(location):
    path = location[0][1:-1]
    suffix = path.split("/")[-2:]
    suffix = "/".join(suffix)
    prefix = "/media/k1462425/BackupsDrive/50KC_flat_with_cloned_postmethod/"
    filepath = prefix + suffix
    lines = []
    with open(filepath,'r',encoding='latin1') as fp:
        line = fp.readline()
        lines.append(line)
        while line:
            line = fp.readline()
            lines.append(line)

    selected_lines = lines[int(location[1][0])-1:int(location[1][1])]
    code = "".join(selected_lines)
    return code


def truncate_and_normalize(projectpairs_edgeweights):
    toremove = []
    # print(projectpairs_edgeweights)
    # sys.exit(0)
    for projectpair in projectpairs_edgeweights:
        # print(projectpairs_edgeweights[projectpair])
        if projectpairs_edgeweights[projectpair] < 0.87:
            toremove.append(projectpair)
    for pair in toremove:
        del projectpairs_edgeweights[pair]

    # print(projectpairs_edgeweights)
    # print("donk")
    # sys.exit(0)

    # for projectpair in projectpairs_edgeweights:
    #     if projectpairs_edgeweights[projectpair] == 1:
    #         projectpairs_edgeweights[projectpair] = 0.999999

    # projectpairs_edgeweights = normalize(projectpairs_edgeweights)

    # print(projectpairs_edgeweights)
    # print("dink")
    # sys.exit(0)
    return projectpairs_edgeweights

def normalize(d, target=1.0):
    raw = sum(d.values())
    factor = target/raw
    return {key:value*factor for key,value in d.items()}

def get_repodirs_and_timespans(repos_and_timespans):
    repodirs_and_timespans = {}
    for repo in repos_and_timespans:
        repodirs_and_timespans[repo.split("/")[-1]] = repos_and_timespans[repo]
    return repodirs_and_timespans


main()
